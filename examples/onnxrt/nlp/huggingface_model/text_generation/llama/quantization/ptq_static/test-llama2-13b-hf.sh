bash run_quant_13b.sh --input_model=/home/azure-node-inc/llama-2-13-hf-onnx --output_model=/home/azure-node-inc/llama-2-13-hf-onnx-inc --weight_only=True --algorithm=GPTQ --tokenizer=/home/azure-node-inc/llama-2-13-hf-onnx --quant_format="QOperator" --workspace=./int4-workspace --scheme=asym --group_size=128
bash run_benchmark.sh --input_model=/home/azure-node-inc/llama-2-13-hf-onnx-inc --mode=accuracy --tokenizer=/home/azure-node-inc/llama-2-13-hf-onnx --tasks=lambada_openai --workspace=./int4-workspace
#bash run_quant_13b.sh --input_model=/home/azure-node-inc/llama-2-13-hf-onnx --output_model=/home/azure-node-inc/llama-2-13-hf-onnx-inc --weight_only=True --algorithm=RTN --tokenizer=/home/azure-node-inc/llama-2-13-hf-onnx --quant_format="QOperator" --workspace=./int4-workspace
#bash run_benchmark.sh --input_model=/home/azure-node-inc/llama-2-13-hf-onnx-inc --mode=accuracy --tokenizer=/home/azure-node-inc/llama-2-13-hf-onnx --tasks=lambada_openai --workspace=./int4-workspace
#bash run_quant_13b.sh --input_model=/home/azure-node-inc/llama-2-13-hf-onnx --output_model=/home/azure-node-inc/llama-2-13-hf-onnx-inc --weight_only=True --algorithm=AWQ --tokenizer=/home/azure-node-inc/llama-2-13-hf-onnx --quant_format="QOperator" --workspace=./int4-workspace
#bash run_benchmark.sh --input_model=/home/azure-node-inc/llama-2-13-hf-onnx-inc --mode=accuracy --tokenizer=/home/azure-node-inc/llama-2-13-hf-onnx --tasks=lambada_openai --workspace=./int4-workspace
