:py:mod:`neural_compressor.config`
==================================

.. py:module:: neural_compressor.config

.. autoapi-nested-parse::

   Configs for Neural Compressor.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   neural_compressor.config.Options
   neural_compressor.config.BenchmarkConfig
   neural_compressor.config.AccuracyCriterion
   neural_compressor.config.TuningCriterion
   neural_compressor.config.PostTrainingQuantConfig
   neural_compressor.config.QuantizationAwareTrainingConfig
   neural_compressor.config.WeightPruningConfig
   neural_compressor.config.KnowledgeDistillationLossConfig
   neural_compressor.config.IntermediateLayersKnowledgeDistillationLossConfig
   neural_compressor.config.SelfKnowledgeDistillationLossConfig
   neural_compressor.config.DistillationConfig
   neural_compressor.config.MixedPrecisionConfig
   neural_compressor.config.ExportConfig
   neural_compressor.config.ONNXQlinear2QDQConfig
   neural_compressor.config.Torch2ONNXConfig
   neural_compressor.config.TF2ONNXConfig



Functions
~~~~~~~~~

.. autoapisummary::

   neural_compressor.config.check_value



.. py:function:: check_value(name, src, supported_type, supported_value=[])

   Check if the given object is the given supported type and in the given supported value.

   Example::

       from neural_compressor.config import check_value

       def datatype(self, datatype):
           if check_value('datatype', datatype, list, ['fp32', 'bf16', 'uint8', 'int8']):
               self._datatype = datatype


.. py:class:: Options(random_seed=1978, workspace=default_workspace, resume_from=None, tensorboard=False)

   Option Class for configs.


.. py:class:: BenchmarkConfig(inputs=[], outputs=[], backend='default', warmup=5, iteration=-1, cores_per_instance=None, num_of_instance=None, inter_num_of_threads=None, intra_num_of_threads=None)

   Config Class for Benchmark.

   :param inputs: A list of strings containing the inputs of model. Default is an empty list.
   :type inputs: list, optional
   :param outputs: A list of strings containing the outputs of model. Default is an empty list.
   :type outputs: list, optional
   :param backend: Backend name for model execution. Supported values include: 'default', 'itex',
                   'ipex', 'onnxrt_trt_ep', 'onnxrt_cuda_ep'. Default value is 'default'.
   :type backend: str, optional
   :param warmup: The number of iterations to perform warmup before running performance tests.
                  Default value is 5.
   :type warmup: int, optional
   :param iteration: The number of iterations to run performance tests. Default is -1.
   :type iteration: int, optional
   :param cores_per_instance: The number of CPU cores to use per instance. Default value is None.
   :type cores_per_instance: int, optional
   :param num_of_instance: The number of instances to use for performance testing.
                           Default value is None.
   :type num_of_instance: int, optional
   :param inter_num_of_threads: The number of threads to use for inter-thread operations.
                                Default value is None.
   :type inter_num_of_threads: int, optional
   :param intra_num_of_threads: The number of threads to use for intra-thread operations.
                                Default value is None.
   :type intra_num_of_threads: int, optional

   Example::

       # Run benchmark according to config

       from neural_compressor.benchmark import fit
       conf = BenchmarkConfig(iteration=100, cores_per_instance=4, num_of_instance=7)
       fit(model='./int8.pb', config=conf, b_dataloader=eval_dataloader)


.. py:class:: AccuracyCriterion(higher_is_better=True, criterion='relative', tolerable_loss=0.01)

   Class of Accuracy Criterion.

   Example::

       from neural_compressor.config import AccuracyCriterion

       accuracy_criterion = AccuracyCriterion(
           higher_is_better=True,  # optional.
           criterion='relative',  # optional. Available values are 'relative' and 'absolute'.
           tolerable_loss=0.01,  # optional.
       )


.. py:class:: TuningCriterion(strategy='basic', strategy_kwargs=None, timeout=0, max_trials=100, objective='performance')

   Class for Tuning Criterion.

   Example::

       from neural_compressor.config import TuningCriterion

       tuning_criterion=TuningCriterion(
           timeout=0, # optional. tuning timeout (seconds). When set to 0, early stopping is enabled.
           max_trials=100, # optional. max tuning times.
                               # combined with the `timeout` field to decide when to exit tuning.
           strategy="basic", # optional. name of the tuning strategy.
           strategy_kwargs=None, # optional. see concrete tuning strategy for available settings.
       )


.. py:class:: PostTrainingQuantConfig(device='cpu', backend='default', domain='auto', recipes={}, quant_format='default', inputs=[], outputs=[], approach='static', calibration_sampling_size=[100], op_type_dict=None, op_name_dict=None, reduce_range=None, excluded_precisions=[], quant_level='auto', tuning_criterion=tuning_criterion, accuracy_criterion=accuracy_criterion, use_distributed_tuning=False)



   Config Class for Post Training Quantization.

   Example::

       from neural_compressor.config PostTrainingQuantConfig, TuningCriterion

       conf = PostTrainingQuantConfig(
           quant_level="auto",  # the quantization level.
           tuning_criterion=TuningCriterion(
               timeout=0,  # optional. tuning timeout (seconds). When set to 0, early stopping is enabled.
               max_trials=100,  # optional. max tuning times.
                               # combined with the `timeout` field to decide when to exit tuning.
           ),
       )


.. py:class:: QuantizationAwareTrainingConfig(device='cpu', backend='default', inputs=[], outputs=[], op_type_dict=None, op_name_dict=None, reduce_range=None, excluded_precisions=[], quant_level='auto')



   Config Class for Quantization Aware Training.

   Example::

       from neural_compressor.config import PostTrainingQuantConfig, QuantizationAwareTrainingConfig

       if approach == "qat":
           model = copy.deepcopy(model_origin)
           conf = QuantizationAwareTrainingConfig(
               op_name_dict=qat_op_name_dict
           )
           compression_manager = prepare_compression(model, conf)


.. py:class:: WeightPruningConfig(pruning_configs=[{}], target_sparsity=0.9, pruning_type='snip_momentum', pattern='4x1', op_names=[], excluded_op_names=[], start_step=0, end_step=0, pruning_scope='global', pruning_frequency=1, min_sparsity_ratio_per_op=0.0, max_sparsity_ratio_per_op=0.98, sparsity_decay_type='exp', pruning_op_types=['Conv', 'Linear'], **kwargs)

   Similiar to torch optimizer's interface.

   Example::

       from neural_compressor.config import WeightPruningConfig

       config = WeightPruningConfig(
           local_configs,
           target_sparsity=0.8
       )
       prune = Pruning(config)
       prune.update_config(start_step=1, end_step=10)
       prune.model = self.model


.. py:class:: KnowledgeDistillationLossConfig(temperature=1.0, loss_types=['CE', 'CE'], loss_weights=[0.5, 0.5])

   Config Class for Knowledge Distillation Loss.

   :param temperature: Hyperparameters that control the entropy
                       of probability distributions. Defaults to 1.0.
   :type temperature: float, optional
   :param loss_types: loss types, should be a list of length 2.
                      First item is the loss type for student model output and groundtruth label,
                      second item is the loss type for student model output and teacher model output.
                      Supported tpyes for first item are "CE", "MSE".
                      Supported tpyes for second item are "CE", "MSE", "KL".
                      Defaults to ['CE', 'CE'].
   :type loss_types: list[str], optional
   :param loss_weights: loss weights, should be a list of length 2 and sum to 1.0.
                        First item is the weight multipled to the loss of student model output and groundtruth label,
                        second item is the weight multipled to the loss of student model output and teacher model output.
                        Defaults to [0.5, 0.5].
   :type loss_weights: list[float], optional

   Example::

       from neural_compressor.config import DistillationConfig, KnowledgeDistillationLossConfig
       from neural_compressor.training import prepare_compression

       criterion_conf = KnowledgeDistillationLossConfig()
       d_conf = DistillationConfig(teacher_model=teacher_model, criterion=criterion_conf)
       compression_manager = prepare_compression(model, d_conf)
       model = compression_manager.model


.. py:class:: IntermediateLayersKnowledgeDistillationLossConfig(layer_mappings=[], loss_types=[], loss_weights=[], add_origin_loss=False)

   Config Class for Intermediate Layers Knowledge Distillation Loss.

   :param layer_mappings: A list for specifying the layer mappings relationship between
                          the student model and the teacher model. Each item in layer_mappings should be a
                          list with the format [(student_layer_name, student_layer_output_process),
                          (teacher_layer_name, teacher_layer_output_process)], where the student_layer_name
                          and the teacher_layer_name are the layer names of the student and the teacher models,
                          e.g. 'bert.layer1.attention'. The student_layer_output_process and teacher_layer_output_process
                          are output process method to get the desired output from the layer specified in the layer
                          name, its value can be either a function or a string, in function case, the function
                          takes output of the specified layer as input, in string case, when output of the
                          specified layer is a dict, this string will serve as key to get corresponding value,
                          when output of the specified layer is a list or tuple, the string should be numeric and
                          will serve as the index to get corresponding value.
                          When output process is not needed, the item in layer_mappings can be abbreviated to
                          [(student_layer_name, ), (teacher_layer_name, )], if student_layer_name and teacher_layer_name
                          are the same, it can be abbreviated further to [(layer_name, )].
                          Some examples of the item in layer_mappings are listed below:
                            [('student_model.layer1.attention', '1'), ('teacher_model.layer1.attention', '1')]
                            [('student_model.layer1.output', ), ('teacher_model.layer1.output', )].
                            [('model.layer1.output', )].
   :type layer_mappings: list
   :param loss_types: loss types, should be a list with the same length of
                      layer_mappings. Each item is the loss type for each layer mapping specified in the
                      layer_mappings. Supported tpyes for each item are "MSE", "KL", "L1". Defaults to
                      ["MSE", ]*len(layer_mappings).
   :type loss_types: list[str], optional
   :param loss_weights: loss weights, should be a list with the same length of
                        layer_mappings. Each item is the weight multipled to the loss of each layer mapping specified
                        in the layer_mappings. Defaults to [1.0 / len(layer_mappings)] * len(layer_mappings).
   :type loss_weights: list[float], optional
   :param add_origin_loss: Whether to add origin loss of the student model. Defaults to False.
   :type add_origin_loss: bool, optional

   Example::

       from neural_compressor.config import DistillationConfig, IntermediateLayersKnowledgeDistillationLossConfig
       from neural_compressor.training import prepare_compression

       criterion_conf = IntermediateLayersKnowledgeDistillationLossConfig(
           layer_mappings=[['layer1.0', ],
                           [['layer1.1.conv1', ], ['layer1.1.conv1', '0']],],
           loss_types=['MSE']*len(layer_mappings),
           loss_weights=[1.0 / len(layer_mappings)]*len(layer_mappings),
           add_origin_loss=True
       )
       d_conf = DistillationConfig(teacher_model=teacher_model, criterion=criterion_conf)
       compression_manager = prepare_compression(model, d_conf)
       model = compression_manager.model


.. py:class:: SelfKnowledgeDistillationLossConfig(layer_mappings=[], temperature=1.0, loss_types=[], loss_weights=[], add_origin_loss=False)

   Config Class for Self Knowledge Distillation Loss.

   :param layer_mappings: layers of distillation. Format like
                          [[[student1_layer_name1, teacher_layer_name1],[student2_layer_name1, teacher_layer_name1]],
                          [[student1_layer_name2, teacher_layer_name2],[student2_layer_name2, teacher_layer_name2]]]
   :type layer_mappings: list
   :param temperature: use to calculate the soft label CE.
   :type temperature: float, optional
   :param loss_types: loss types, should be a list with the same length of
                      layer_mappings. Each item is the loss type for each layer mapping specified in the
                      layer_mappings. Supported tpyes for each item are "CE", "KL", "L2". Defaults to
                      ["CE", ]*len(layer_mappings).
   :type loss_types: list, optional
   :param loss_weights: loss weights. Defaults to [1.0 / len(layer_mappings)] *
                        len(layer_mappings).
   :type loss_weights: list, optional
   :param add_origin_loss: whether to add origin loss for hard label loss.
   :type add_origin_loss: bool, optional

   Example::

       from neural_compressor.training import prepare_compression
       from neural_compressor.config import DistillationConfig, SelfKnowledgeDistillationLossConfig

       criterion_conf = SelfKnowledgeDistillationLossConfig(
           layer_mappings=[
               [['resblock.1.feature.output', 'resblock.deepst.feature.output'],
               ['resblock.2.feature.output','resblock.deepst.feature.output']],
               [['resblock.2.fc','resblock.deepst.fc'],
               ['resblock.3.fc','resblock.deepst.fc']],
               [['resblock.1.fc','resblock.deepst.fc'],
               ['resblock.2.fc','resblock.deepst.fc'],
               ['resblock.3.fc','resblock.deepst.fc']]
           ],
           temperature=3.0,
           loss_types=['L2', 'KL', 'CE'],
           loss_weights=[0.5, 0.05, 0.02],
           add_origin_loss=True,)
       conf = DistillationConfig(teacher_model=model, criterion=criterion_conf)
       criterion = nn.CrossEntropyLoss()
       optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)
       compression_manager = prepare_compression(model, conf)
       model = compression_manager.model


.. py:class:: DistillationConfig(teacher_model=None, criterion=criterion, optimizer={'SGD': {'learning_rate': 0.0001}})

   Config of distillation.

   :param teacher_model: Teacher model for distillation. Defaults to None.
   :type teacher_model: Callable
   :param features: Teacher features for distillation, features and teacher_model are alternative.
                    Defaults to None.
   :type features: optional
   :param criterion: Distillation loss configure.
   :type criterion: Callable, optional
   :param optimizer: Optimizer configure.
   :type optimizer: dictionary, optional

   Example::

       from neural_compressor.training import prepare_compression
       from neural_compressor.config import DistillationConfig, SelfKnowledgeDistillationLossConfig

       distil_loss = SelfKnowledgeDistillationLossConfig()
       conf = DistillationConfig(teacher_model=model, criterion=distil_loss)
       criterion = nn.CrossEntropyLoss()
       optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)
       compression_manager = prepare_compression(model, conf)
       model = compression_manager.model


.. py:class:: MixedPrecisionConfig(device='cpu', backend='default', precision='bf16', inputs=[], outputs=[], tuning_criterion=tuning_criterion, accuracy_criterion=accuracy_criterion, excluded_precisions=[])



   Config Class for MixedPrecision.

   :param device: Device for execution.
                  Support 'cpu' and 'gpu', default is 'cpu'.
   :type device: str, optional
   :param backend: Backend for model execution.
                   Support 'default', 'itex', 'ipex', 'onnxrt_trt_ep', 'onnxrt_cuda_ep',
                   default is 'default'.
   :type backend: str, optional
   :param precision: Target precision for mix precision conversion.
                     Support 'bf16' and 'fp16', default is 'bf16'.
   :type precision: str, optional
   :param inputs: Inputs of model, default is [].
   :type inputs: list, optional
   :param outputs: Outputs of model, default is [].
   :type outputs: list, optional
   :param tuning_criterion: Accuracy tuning settings,
                            it won't work if there is no accuracy tuning process.
   :type tuning_criterion: TuningCriterion object, optional
   :param accuracy_criterion: Accuracy constraint settings,
                              it won't work if there is no accuracy tuning process.
   :type accuracy_criterion: AccuracyCriterion object, optional
   :param excluded_precisions: Precisions to be excluded during mix precision conversion, default is [].
   :type excluded_precisions: list, optional

   Example::

       from neural_compressor import mix_precision
       from neural_compressor.config import MixedPrecisionConfig

       conf = MixedPrecisionConfig()
       converted_model = mix_precision.fit(model, config=conf)


.. py:class:: ExportConfig(dtype='int8', opset_version=14, quant_format='QDQ', example_inputs=None, input_names=None, output_names=None, dynamic_axes=None)

   Config Class for Export.


.. py:class:: ONNXQlinear2QDQConfig

   Config Class for ONNXQlinear2QDQ.

   Example::

       from neural_compressor.config import ONNXQlinear2QDQConfig
       from neural_compressor.model import Model

       conf = ONNXQlinear2QDQConfig()
       model = Model(model)
       model.export('new_model.onnx', conf)


.. py:class:: Torch2ONNXConfig(dtype='int8', opset_version=14, quant_format='QDQ', example_inputs=None, input_names=None, output_names=None, dynamic_axes=None, recipe='QDQ_OP_FP32_BIAS', **kwargs)



   Config Class for Torch2ONNX.


.. py:class:: TF2ONNXConfig(dtype='int8', opset_version=14, quant_format='QDQ', example_inputs=None, input_names=None, output_names=None, dynamic_axes=None, **kwargs)



   Config Class for TF2ONNX.

   :param dtype: The data type of export target model. Supports 'fp32' and 'int8'.
                 Defaults to 'int8'.
   :type dtype: str, optional
   :param opset_version: The version of the ONNX operator set to use. Defaults to 14.
   :type opset_version: int, optional
   :param quant_format: The quantization format for the export target model.
                        Supports 'default', 'QDQ' and 'QOperator'. Defaults to 'QDQ'.
   :type quant_format: str, optional
   :param example_inputs: A list example inputs to use for tracing the model.
                          Defaults to None.
   :type example_inputs: list, optional
   :param input_names: A list of model input names. Defaults to None.
   :type input_names: list, optional
   :param output_names: A list of model output names. Defaults to None.
   :type output_names: list, optional
   :param dynamic_axes: A dictionary of dynamic axis information. Defaults to None.
   :type dynamic_axes: dict, optional
   :param \*\*kwargs: Additional keyword arguments.

   Examples::

       # tensorflow QDQ int8 model 'q_model' export to ONNX int8 model
       from neural_compressor.config import TF2ONNXConfig
       config = TF2ONNXConfig()
       q_model.export(output_graph, config)


