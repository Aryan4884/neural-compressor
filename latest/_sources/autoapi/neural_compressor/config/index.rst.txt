:py:mod:`neural_compressor.config`
==================================

.. py:module:: neural_compressor.config

.. autoapi-nested-parse::

   Configs for Neural Compressor.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   neural_compressor.config.Options
   neural_compressor.config.BenchmarkConfig
   neural_compressor.config.AccuracyCriterion
   neural_compressor.config.TuningCriterion
   neural_compressor.config.PostTrainingQuantConfig
   neural_compressor.config.QuantizationAwareTrainingConfig
   neural_compressor.config.WeightPruningConfig
   neural_compressor.config.KnowledgeDistillationLossConfig
   neural_compressor.config.IntermediateLayersKnowledgeDistillationLossConfig
   neural_compressor.config.SelfKnowledgeDistillationLossConfig
   neural_compressor.config.DistillationConfig
   neural_compressor.config.MixedPrecisionConfig
   neural_compressor.config.ExportConfig
   neural_compressor.config.ONNXQlinear2QDQConfig
   neural_compressor.config.Torch2ONNXConfig
   neural_compressor.config.TF2ONNXConfig



Functions
~~~~~~~~~

.. autoapisummary::

   neural_compressor.config.check_value



.. py:function:: check_value(name, src, supported_type, supported_value=[])

   Check if the given object is the given supported type and in the given supported value.

   Example::

       from neural_compressor.config import check_value

       def datatype(self, datatype):
           if check_value('datatype', datatype, list, ['fp32', 'bf16', 'uint8', 'int8']):
               self._datatype = datatype


.. py:class:: Options(random_seed=1978, workspace=default_workspace, resume_from=None, tensorboard=False)

   Option Class for configs.


.. py:class:: BenchmarkConfig(inputs=[], outputs=[], backend='default', warmup=5, iteration=-1, cores_per_instance=None, num_of_instance=None, inter_num_of_threads=None, intra_num_of_threads=None)

   Config Class for Benchmark.

   :param inputs: A list of strings containing the inputs of model. Default is an empty list.
   :type inputs: list, optional
   :param outputs: A list of strings containing the outputs of model. Default is an empty list.
   :type outputs: list, optional
   :param backend: Backend name for model execution. Supported values include: 'default', 'itex',
                   'ipex', 'onnxrt_trt_ep', 'onnxrt_cuda_ep'. Default value is 'default'.
   :type backend: str, optional
   :param warmup: The number of iterations to perform warmup before running performance tests.
                  Default value is 5.
   :type warmup: int, optional
   :param iteration: The number of iterations to run performance tests. Default is -1.
   :type iteration: int, optional
   :param cores_per_instance: The number of CPU cores to use per instance. Default value is None.
   :type cores_per_instance: int, optional
   :param num_of_instance: The number of instances to use for performance testing.
                           Default value is None.
   :type num_of_instance: int, optional
   :param inter_num_of_threads: The number of threads to use for inter-thread operations.
                                Default value is None.
   :type inter_num_of_threads: int, optional
   :param intra_num_of_threads: The number of threads to use for intra-thread operations.
                                Default value is None.
   :type intra_num_of_threads: int, optional

   Example::

       # Run benchmark according to config

       from neural_compressor.benchmark import fit
       conf = BenchmarkConfig(iteration=100, cores_per_instance=4, num_of_instance=7)
       fit(model='./int8.pb', config=conf, b_dataloader=eval_dataloader)


.. py:class:: AccuracyCriterion(higher_is_better=True, criterion='relative', tolerable_loss=0.01)

   Class of Accuracy Criterion.

   Example::

       from neural_compressor.config import AccuracyCriterion

       accuracy_criterion = AccuracyCriterion(
           higher_is_better=True,  # optional.
           criterion='relative',  # optional. Available values are 'relative' and 'absolute'.
           tolerable_loss=0.01,  # optional.
       )


.. py:class:: TuningCriterion(strategy='basic', strategy_kwargs=None, timeout=0, max_trials=100, objective='performance')

   Class for Tuning Criterion.

   Example::

       from neural_compressor.config import TuningCriterion

       tuning_criterion=TuningCriterion(
           timeout=0, # optional. tuning timeout (seconds). When set to 0, early stopping is enabled.
           max_trials=100, # optional. max tuning times.
                               # combined with the `timeout` field to decide when to exit tuning.
           strategy="basic", # optional. name of the tuning strategy.
           strategy_kwargs=None, # optional. see concrete tuning strategy for available settings.
       )


.. py:class:: PostTrainingQuantConfig(device='cpu', backend='default', domain='auto', recipes={}, quant_format='default', inputs=[], outputs=[], approach='static', calibration_sampling_size=[100], op_type_dict=None, op_name_dict=None, reduce_range=None, excluded_precisions=[], quant_level='auto', tuning_criterion=tuning_criterion, accuracy_criterion=accuracy_criterion, use_distributed_tuning=False)



   Config Class for Post Training Quantization.

   Example::

       from neural_compressor.config PostTrainingQuantConfig, TuningCriterion

       conf = PostTrainingQuantConfig(
           quant_level="auto",  # the quantization level.
           tuning_criterion=TuningCriterion(
               timeout=0,  # optional. tuning timeout (seconds). When set to 0, early stopping is enabled.
               max_trials=100,  # optional. max tuning times.
                               # combined with the `timeout` field to decide when to exit tuning.
           ),
       )


.. py:class:: QuantizationAwareTrainingConfig(device='cpu', backend='default', inputs=[], outputs=[], op_type_dict=None, op_name_dict=None, reduce_range=None, excluded_precisions=[], quant_level='auto')



   Config Class for Quantization Aware Training.

   Example::

       from neural_compressor.config import PostTrainingQuantConfig, QuantizationAwareTrainingConfig

       if approach == "qat":
           model = copy.deepcopy(model_origin)
           conf = QuantizationAwareTrainingConfig(
               op_name_dict=qat_op_name_dict
           )
           compression_manager = prepare_compression(model, conf)


.. py:class:: WeightPruningConfig(pruning_configs=[{}], target_sparsity=0.9, pruning_type='snip_momentum', pattern='4x1', op_names=[], excluded_op_names=[], start_step=0, end_step=0, pruning_scope='global', pruning_frequency=1, min_sparsity_ratio_per_op=0.0, max_sparsity_ratio_per_op=0.98, sparsity_decay_type='exp', pruning_op_types=['Conv', 'Linear'], **kwargs)

   Similiar to torch optimizer's interface.

   Example::

       from neural_compressor.config import WeightPruningConfig

       config = WeightPruningConfig(
           local_configs,
           target_sparsity=0.8
       )
       prune = Pruning(config)
       prune.update_config(start_step=1, end_step=10)
       prune.model = self.model


.. py:class:: KnowledgeDistillationLossConfig(temperature=1.0, loss_types=['CE', 'CE'], loss_weights=[0.5, 0.5])

   Config Class for Knowledge Distillation Loss.

   Example::

       from neural_compressor.config import DistillationConfig, KnowledgeDistillationLossConfig
       from neural_compressor import QuantizationAwareTrainingConfig
       from neural_compressor.training import prepare_compression

       combs = []
       distillation_criterion = KnowledgeDistillationLossConfig()
       d_conf = DistillationConfig(teacher_model=teacher_model, criterion=distillation_criterion)
       combs.append(d_conf)
       q_conf = QuantizationAwareTrainingConfig()
       combs.append(q_conf)
       compression_manager = prepare_compression(model, combs)
       model = compression_manager.model


.. py:class:: IntermediateLayersKnowledgeDistillationLossConfig(layer_mappings=[], loss_types=[], loss_weights=[], add_origin_loss=False)

   Config Class for Intermediate Layers Knowledge Distillation Loss.

   Example::

       from neural_compressor.config import DistillationConfig, IntermediateLayersKnowledgeDistillationLossConfig

       distillation_criterion = IntermediateLayersKnowledgeDistillationLossConfig(
           layer_mappings=layer_mappings,
           loss_types=['MSE']*len(layer_mappings),
           loss_weights=[1.0 / len(layer_mappings)]*len(layer_mappings),
           add_origin_loss=True
       )
       d_conf = DistillationConfig(teacher_model=teacher_model, criterion=distillation_criterion)
       confs.append(d_conf)


.. py:class:: SelfKnowledgeDistillationLossConfig(layer_mappings=[], temperature=1.0, loss_types=[], loss_weights=[], add_origin_loss=False)

   Config Class for Self Knowledge Distillation Loss.

   Example::

       from neural_compressor.training import prepare_compression
       from neural_compressor.config import DistillationConfig, SelfKnowledgeDistillationLossConfig

       distil_loss = SelfKnowledgeDistillationLossConfig()
       conf = DistillationConfig(teacher_model=model, criterion=distil_loss)
       criterion = nn.CrossEntropyLoss()
       optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)
       compression_manager = prepare_compression(model, conf)
       model = compression_manager.model


.. py:class:: DistillationConfig(teacher_model=None, criterion=criterion, optimizer={'SGD': {'learning_rate': 0.0001}})

   Config of distillation.

   :param teacher_model: Teacher model for distillation. Defaults to None.
   :type teacher_model: Callable
   :param features: Teacher features for distillation, features and teacher_model are alternative.
                    Defaults to None.
   :type features: optional
   :param criterion: Distillation loss configure.
   :type criterion: Callable, optional
   :param optimizer: Optimizer configure.
   :type optimizer: dictionary, optional

   Example::

       from neural_compressor.training import prepare_compression
       from neural_compressor.config import DistillationConfig, SelfKnowledgeDistillationLossConfig

       distil_loss = SelfKnowledgeDistillationLossConfig()
       conf = DistillationConfig(teacher_model=model, criterion=distil_loss)
       criterion = nn.CrossEntropyLoss()
       optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)
       compression_manager = prepare_compression(model, conf)
       model = compression_manager.model


.. py:class:: MixedPrecisionConfig(device='cpu', backend='default', precision='bf16', inputs=[], outputs=[], tuning_criterion=tuning_criterion, accuracy_criterion=accuracy_criterion, excluded_precisions=[])



   Config Class for MixedPrecision.

   :param device: device for execution. Support 'cpu' and 'gpu', default is 'cpu'
   :type device: str, optional
   :param backend: backend for model execution. Support 'default', 'itex', 'ipex',
                   'onnxrt_trt_ep', 'onnxrt_cuda_ep', default is 'default'
   :type backend: str, optional
   :param precision: target precision for mix precision conversion.
                     Support 'bf16' and 'fp16', default is 'bf16'
   :type precision: str, optional
   :param inputs: inputs of model, default is []
   :type inputs: list, optional
   :param outputs: outputs of model, default is []
   :type outputs: list, optional
   :param tuning_criterion: accuracy tuning settings, it won't work
                            if there is no accuracy tuning process
   :type tuning_criterion: TuningCriterion object, optional
   :param accuracy_criterion: accuracy constraint settings, it won't
                              work if there is no accuracy tuning process
   :type accuracy_criterion: AccuracyCriterion object, optional
   :param excluded_precisions: precisions to be excluded during mix precision conversion,
                               default is []
   :type excluded_precisions: list, optional

   Example::

       from neural_compressor import mix_precision
       from neural_compressor.config import MixedPrecisionConfig

       conf = MixedPrecisionConfig()
       converted_model = mix_precision.fit(model, config=conf)


.. py:class:: ExportConfig(dtype='int8', opset_version=14, quant_format='QDQ', example_inputs=None, input_names=None, output_names=None, dynamic_axes=None)

   Config Class for Export.


.. py:class:: ONNXQlinear2QDQConfig

   Config Class for ONNXQlinear2QDQ.

   Example::

       from neural_compressor.config import ONNXQlinear2QDQConfig
       from neural_compressor.model import Model

       conf = ONNXQlinear2QDQConfig()
       model = Model(model)
       model.export('new_model.onnx', conf)


.. py:class:: Torch2ONNXConfig(dtype='int8', opset_version=14, quant_format='QDQ', example_inputs=None, input_names=None, output_names=None, dynamic_axes=None, recipe='QDQ_OP_FP32_BIAS', **kwargs)



   Config Class for Torch2ONNX.


.. py:class:: TF2ONNXConfig(dtype='int8', opset_version=14, quant_format='QDQ', example_inputs=None, input_names=None, output_names=None, dynamic_axes=None, **kwargs)



   Config Class for TF2ONNX.

   :param dtype: The data type of export target model. Supports 'fp32' and 'int8'.
                 Defaults to 'int8'.
   :type dtype: str, optional
   :param opset_version: The version of the ONNX operator set to use. Defaults to 14.
   :type opset_version: int, optional
   :param quant_format: The quantization format for the export target model.
                        Supports 'default', 'QDQ' and 'QOperator'. Defaults to 'QDQ'.
   :type quant_format: str, optional
   :param example_inputs: A list example inputs to use for tracing the model.
                          Defaults to None.
   :type example_inputs: list, optional
   :param input_names: A list of model input names. Defaults to None.
   :type input_names: list, optional
   :param output_names: A list of model output names. Defaults to None.
   :type output_names: list, optional
   :param dynamic_axes: A dictionary of dynamic axis information. Defaults to None.
   :type dynamic_axes: dict, optional
   :param \*\*kwargs: Additional keyword arguments.

   Examples::

       # tensorflow QDQ int8 model 'q_model' export to ONNX int8 model
       from neural_compressor.config import TF2ONNXConfig
       config = TF2ONNXConfig()
       q_model.export(output_graph, config)


