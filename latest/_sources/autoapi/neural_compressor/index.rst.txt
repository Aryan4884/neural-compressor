:py:mod:`neural_compressor`
===========================

.. py:module:: neural_compressor

.. autoapi-nested-parse::

   IntelÂ® Neural Compressor: An open-source Python library supporting popular model compression techniques.



Subpackages
-----------
.. toctree::
   :titlesonly:
   :maxdepth: 3

   algorithm/index.rst
   contrib/index.rst
   data/index.rst
   experimental/index.rst
   metric/index.rst
   model/index.rst
   strategy/index.rst
   utils/index.rst
   ux/index.rst


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   benchmark/index.rst
   config/index.rst
   mix_precision/index.rst
   objective/index.rst
   quantization/index.rst
   training/index.rst
   version/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   neural_compressor.Benchmark
   neural_compressor.DistillationConfig
   neural_compressor.PostTrainingQuantConfig
   neural_compressor.WeightPruningConfig
   neural_compressor.QuantizationAwareTrainingConfig
   neural_compressor.MixedPrecisionConfig



Functions
~~~~~~~~~

.. autoapisummary::

   neural_compressor.set_random_seed
   neural_compressor.set_tensorboard
   neural_compressor.set_workspace



.. py:class:: Benchmark(conf)

   Bases: :py:obj:`object`

   Benchmark class can be used to evaluate the model performance.

   With the objective setting, user can get the data of what they configured in yaml.

   :param conf: The config.BenchmarkConfig class containing accuracy goal, tuning objective etc.
   :type conf: obj

   .. py:property:: results

      Get the results of benchmarking.

   .. py:property:: b_dataloader

      Get the dataloader for the benchmarking.

   .. py:property:: b_func

      Not support getting b_func.

   .. py:property:: model

      Get the model.

   .. py:method:: summary_benchmark()

      Get the summary of the benchmark.


   .. py:method:: config_instance(raw_cmd)

      Configure the multi-instance commands and trigger benchmark with sub process.

      :param raw_cmd: raw command used for benchmark


   .. py:method:: generate_prefix(core_list)

      Generate the command prefix with numactl.

      :param core_list: a list of core indexes bound with specific instances


   .. py:method:: run_instance()

      Run the instance with the configuration.

      :param runs benchmarking with numactl on specific cores and instances set: by user config and returns model performance



.. py:function:: set_random_seed(seed: int)

   Set the random seed in config.


.. py:function:: set_tensorboard(tensorboard: bool)

   Set the tensorboard in config.


.. py:function:: set_workspace(workspace: str)

   Set the workspace in config.


.. py:class:: DistillationConfig(teacher_model=None, criterion=criterion, optimizer={'SGD': {'learning_rate': 0.0001}})

   Config of distillation.

   :param teacher_model: Teacher model for distillation. Defaults to None.
   :type teacher_model: Callable
   :param features: Teacher features for distillation, features and teacher_model are alternative.
                    Defaults to None.
   :type features: optional
   :param criterion: Distillation loss configure.
   :type criterion: Callable, optional
   :param optimizer: Optimizer configure.
   :type optimizer: dictionary, optional

   .. rubric:: Example

   from neural_compressor.training import prepare_compression
   from neural_compressor.config import DistillationConfig, SelfKnowledgeDistillationLossConfig

   distil_loss = SelfKnowledgeDistillationLossConfig()
   conf = DistillationConfig(teacher_model=model, criterion=distil_loss)
   criterion = nn.CrossEntropyLoss()
   optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)
   compression_manager = prepare_compression(model, conf)
   model = compression_manager.model

   .. py:property:: criterion

      Get criterion.

   .. py:property:: optimizer

      Get optimizer.

   .. py:property:: teacher_model

      Get teacher_model.


.. py:class:: PostTrainingQuantConfig(device='cpu', backend='default', domain='auto', recipes={}, quant_format='default', inputs=[], outputs=[], approach='static', calibration_sampling_size=[100], op_type_dict=None, op_name_dict=None, reduce_range=None, excluded_precisions=[], quant_level='auto', tuning_criterion=tuning_criterion, accuracy_criterion=accuracy_criterion, use_distributed_tuning=False)

   Bases: :py:obj:`_BaseQuantizationConfig`

   Config Class for Post Training Quantization.

   .. rubric:: Example

   from neural_compressor.config PostTrainingQuantConfig, TuningCriterion

   conf = PostTrainingQuantConfig(
       quant_level="auto",  # the quantization level.
       tuning_criterion=TuningCriterion(
           timeout=0,  # optional. tuning timeout (seconds). When set to 0, early stopping is enabled.
           max_trials=100,  # optional. max tuning times.
                           # combined with the `timeout` field to decide when to exit tuning.
       ),
   )

   .. py:property:: approach

      Get approach.

   .. py:property:: tuning_criterion

      Get tuning_criterion.


.. py:class:: WeightPruningConfig(pruning_configs=[{}], target_sparsity=0.9, pruning_type='snip_momentum', pattern='4x1', op_names=[], excluded_op_names=[], start_step=0, end_step=0, pruning_scope='global', pruning_frequency=1, min_sparsity_ratio_per_op=0.0, max_sparsity_ratio_per_op=0.98, sparsity_decay_type='exp', pruning_op_types=['Conv', 'Linear'], **kwargs)

   Similiar to torch optimizer's interface.

   .. rubric:: Example

   from neural_compressor.config import WeightPruningConfig

   config = WeightPruningConfig(
       local_configs,
       target_sparsity=0.8
   )
   prune = Pruning(config)
   prune.update_config(start_step=1, end_step=10)
   prune.model = self.model

   .. py:property:: weight_compression

      Get weight_compression.


.. py:class:: QuantizationAwareTrainingConfig(device='cpu', backend='default', inputs=[], outputs=[], op_type_dict=None, op_name_dict=None, reduce_range=None, excluded_precisions=[], quant_level='auto')

   Bases: :py:obj:`_BaseQuantizationConfig`

   Config Class for Quantization Aware Training.

   .. rubric:: Example

   from neural_compressor.config import PostTrainingQuantConfig, QuantizationAwareTrainingConfig

   if approach == "qat":
       model = copy.deepcopy(model_origin)
       conf = QuantizationAwareTrainingConfig(
           op_name_dict=qat_op_name_list
       )
       compression_manager = prepare_compression(model, conf)

   .. py:property:: approach

      Get approach.


.. py:class:: MixedPrecisionConfig(device='cpu', backend='default', inputs=[], outputs=[], tuning_criterion=tuning_criterion, accuracy_criterion=accuracy_criterion, excluded_precisions=[])

   Bases: :py:obj:`PostTrainingQuantConfig`

   Config Class for MixedPrecision.

   .. rubric:: Example

   from neural_compressor import mix_precision
   from neural_compressor.config import MixedPrecisionConfig

   conf = MixedPrecisionConfig()
   converted_model = mix_precision.fit(model, config=conf)


