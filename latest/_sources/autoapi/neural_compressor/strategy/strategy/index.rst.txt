:py:mod:`neural_compressor.strategy.strategy`
=============================================

.. py:module:: neural_compressor.strategy.strategy

.. autoapi-nested-parse::

   The base class for tuning strategy.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   neural_compressor.strategy.strategy.TuneStrategy



Functions
~~~~~~~~~

.. autoapisummary::

   neural_compressor.strategy.strategy.strategy_registry



.. py:function:: strategy_registry(cls)

   Class decorator used to register all TuneStrategy subclasses.

   :param cls: The class of register.
   :type cls: class

   :returns: The class of register.
   :rtype: cls


.. py:class:: TuneStrategy(model, conf, q_dataloader=None, q_func=None, eval_dataloader=None, eval_func=None, resume=None, q_hooks=None)

   Bases: :py:obj:`object`

   Basic class for tuning strategy.

   .. py:property:: evaluation_result

      Evaluate the given model.

      :returns: The objective value evaluated.

   .. py:method:: next_tune_cfg()
      :abstractmethod:

      Interface for generate the next tuning config.

      The generator of yielding next tuning config to traverse by concrete strategies or quantization level
      according to last tuning result and traverse logic.

      It should be implemented by the sub-class.

      :Yields: *tune_config (dict)* -- It's a dict containing the tuning configuration to traverse.


   .. py:method:: distributed_next_tune_cfg_lst(comm)

      Interface for generate the distributed next tuning config list.

      The generator of yielding next tuning config list to distributed traverse by concrete strategies or
      quantization level according to tuning result and traverse logic.

      It should be implemented by the sub-class. Currently, it is only implemented in the BasicTuneStrategy.


   .. py:method:: meet_acc_req(eval_res)

      Compare the result of last tuning with baseline to check whether the result meet requirements.

      :param eval_res: The evaluation result of tuning.

      :returns: Return True if the accuracy meets requirements else False.


   .. py:method:: master_worker_handle(comm)

      Matster worker handles the task assignment and result management.

      Master node send all task ids to all free nodes, and wait until any result.
      When receiving any result, directly send a new task id to the sender (it's free).

      :param comm: The instance of comunication for MPI.
      :type comm: MPI.COMM


   .. py:method:: slave_worker_handle(comm)

      Slave worker handles the task processing.

      When receiving any task id, slave node finds it in self.tune_cfg_lst and run it.
      Then slave node sends back the tune result to master node.

      :param comm: The instance of comunication for MPI.
      :type comm: MPI.COMM


   .. py:method:: distributed_traverse()

      Disributed traverse the tuning space.

      The main traverse logic which could be override by some concrete strategy which needs more hooks.


   .. py:method:: apply_all_tuning_recipes(tune_cfg)

      Apply all tunable recipes with their value.


   .. py:method:: apply_recipe_one_by_one(tune_cfg)

      Apply the tunable recipes one by one.

      For recipes only have two options, apply the last one.
      For recipes with multiple values. such as alpha of smooth quant, apply it one by one.


   .. py:method:: set_param_for_pre_quantization_algos(algo_scheduler, tune_cfg, fp32_model) -> None

      Set the parameter for pre-quantization algos, such as smooth quantization.

      :param algo_scheduler: algo scheduler
      :param tune_cfg: the tuning config
      :param fp32_model: the fp32 model


   .. py:method:: set_param_for_post_quantization_algos(algo_scheduler, tune_cfg, pre_optimized_model, q_model) -> None

      Set the parameter for post-quantization algos, such as bias correction, weight correction.

      :param algo_scheduler: algo scheduler
      :param tune_cfg: the tuning config.
      :param pre_optimized_model: the pre-optimized model
      :param q_model: the quantized model


   .. py:method:: traverse()

      Traverse the tuning space.

      The main traverse logic which could be override by some concrete strategy which needs more hooks.


   .. py:method:: initial_tuning_cfg()

      Init the tuning config.

      Initialize the tuning config according to the quantization approach.

      :returns: key is (op_name, op_type); value is quantization mode.
                quant_mode_wise_items (OrderedDict): key is quant_mode/precision; value is item list.
                initial_op_tuning_cfg (OrderedDict): key is (op_name, op_type); value is the initialized tuning config.
      :rtype: op_item_dtype_dict (OrderedDict)


   .. py:method:: show_baseline_info()

      Display the accuracy and duration of the the baseline model.


   .. py:method:: initial_best_acc()

      Init the best accuracy.

      :returns: The initial value of best accuracy.


   .. py:method:: set_tuning_space(conf)

      Create the tuning space.

      Create the tuning space based on the framework capability and user configuration.

      :param conf: The Conf class instance includes all user configurations.


   .. py:method:: setup_resume(resume)

      Resume the best quantized model from tuning history.

      :param resume: The dict containing resume information.


   .. py:method:: set_q_func()

      Set the training function for quantization aware training.


   .. py:method:: update_best_op_tuning_cfg(op_tuning_cfg)

      Track and update the best tuning config with correspondence accuracy result.

      :param op_tuning_cfg: The tuning config.

      :returns: The current best tuning results and corresponding configurations.


   .. py:method:: deploy_config()

      Save the configuration locally for deployment.


   .. py:method:: stop(timeout, trials_count)

      Check if need to stop traverse.

      Check if need to stop traversing the tuning space, either accuracy goal is met or timeout is reach.

      :returns: True if need stop, otherwise False
      :rtype: bool



