:py:mod:`neural_compressor.strategy.conservative`
=================================================

.. py:module:: neural_compressor.strategy.conservative

.. autoapi-nested-parse::

   The conservative tuning strategy for quantization level 0.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   neural_compressor.strategy.conservative.ConservativeTuneStrategy




.. py:class:: ConservativeTuneStrategy(model, conf, q_dataloader, q_func=None, eval_dataloader=None, eval_func=None, dicts=None, q_hooks=None)

   Bases: :py:obj:`neural_compressor.strategy.strategy.TuneStrategy`

   Tuning strategy with accuracy first, performance second.

   The quantization level O0 is designed for user who want to keep the accuracy
   of the model after quantization. It starts with the original(fp32) model,
   and then quantize the OPs to lower precision OP type wisely and OP wisely.

   ..    .. py:method:: next_tune_cfg()

      Generate and yield the next tuning config with below order.

      1. Query all quantifiable ops and save as a list of [(op_name, op_type), ...]
      2. Classify the op by its op type
      3. Add op to quant_queue according to the op type priority
      4. Go through the quant_queue and replace it with the fp32 config in tune_cfg if
      accuracy meets the requirements else continue
      5. For bf16 and fp16 operators, do the same as int8 operators.

      :returns: It's a dict containing the tuning configuration to run.
      :rtype: tune_config (dict)


   .. py:method:: initialize_tune_cfg()

      Init the tuning config.

      Initialize the tuning config for conservative tuning.

      :returns: key is (op_name, op_type); value is quantization mode.
                quant_mode_wise_items (OrderedDict): key is quant_mode/precision; value is item list.
                initial_op_tuning_cfg (OrderedDict): key is (op_name, op_type); value is the initialized tuning config.
      :rtype: op_item_dtype_dict (OrderedDict)



